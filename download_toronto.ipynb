{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *** CHANGE THIS as appropriate ***:\n",
    "# Base path to save all data to\n",
    "my_base_path = \"/home/sergey/Desktop/HackOnData/downloaded_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INPUT: URL to json file\n",
    "# OUTPUT: json data\n",
    "def get_jsonparsed_data(json_url):\n",
    "    response = urllib.request.urlopen(json_url)\n",
    "    str_response = response.read().decode('utf-8')\n",
    "    return json.loads(str_response)\n",
    "\n",
    "\n",
    "# INPUT: html from toronto.ca\n",
    "# OUTPUT: metadata in our format\n",
    "def create_metadata(html):\n",
    "#     html = urllib.request.urlopen(toronto_URLs[idx]).read()\n",
    "    my_metadata = {} # create empty dict to be filled with metadata\n",
    "#     my_metadata['title'] = re.findall(r'<li class=\"active\">(.+?)</li>\\\\r\\\\n\\\\r\\\\n\\\\t\\\\t\\\\t\\\\t</ol>', str(html))\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    my_metadata['title'] = urlify_title(soup.title.string)\n",
    "    my_metadata['source_page'] = toronto_URLs[idx]\n",
    "    my_metadata['update_frequency'] = re.findall(r'Refresh rate.+?<dd>(.+?)</dd>', str(html));\n",
    "    \n",
    "    # fields below still need to be filled with actual values\n",
    "    my_metadata['source_files'] = [] # ['http://url_to_source_file_1','http://url_to_source_file_2']\n",
    "    my_metadata['data_last_modified'] = '' # '<2016-06-30>'\n",
    "    my_metadata['Category'] = 'Public Administration'\n",
    "    my_metadata['data_schema'] = {'field_1': 'type:<string/integer/decimal/date>'}\n",
    "    my_metadata['description'] = 'A description'\n",
    "    my_metadata['license'] = 'open'\n",
    "    my_metadata['tags'] = ['tag_1', 'tag_2', 'tag_3']\n",
    "    return my_metadata\n",
    "\n",
    "    \n",
    "# Saves file from URL to folder_name, using specified file_name or automatically assigned one.\n",
    "# INPUT: URL; folder_name where file will be saved; file_name = 0 for automatic assignment; rewrite = 0, overwrite existing file\n",
    "def download_file(URL, folder_name, file_name = 0, rewrite = 0): \n",
    "    if file_name == 0: # if file name is not specified\n",
    "        file_name = os.path.basename(URL) # get file name\n",
    "    full_path_to_save = os.path.join(folder_name, file_name)\n",
    "    if not rewrite: # then check if exists\n",
    "        if os.path.isfile(full_path_to_save):\n",
    "            print('File already exists, skipping...')\n",
    "            return # don't retrive file if it already exists\n",
    "    try: # download\n",
    "        urllib.request.urlretrieve(URL, full_path_to_save)\n",
    "    except ( urllib.request.HTTPError, urllib.request.URLError ): # If unable to download, save failed URL to download_errors.txt\n",
    "        print('    Download ERROR, skipping...')\n",
    "        f = open(os.path.join(folder_name, 'download_errors.txt'), 'a')\n",
    "        f.write(URL + '\\n')\n",
    "        f.close()        \n",
    "\n",
    "        \n",
    "# INPUT: Name of file with URLs. Finds URLs that point to toronto.ca.\n",
    "# OUTPUT: URLs to toronto.ca web pages, and toronto.ca IDs.\n",
    "def get_toronto_urls(hyperlinks_text_file_name):\n",
    "    f = open(hyperlinks_text_file_name, 'r')\n",
    "    toronto_URLs = []; # corresponding list of URLs to toronto.ca data\n",
    "    toronto_IDs = []; # corresponding IDs\n",
    "    for line in f:\n",
    "#         print(line,end='')\n",
    "        match = re.search('toronto.ca', line) # match = re.search(pat, text)\n",
    "        if match:\n",
    "            ID = re.findall('toid=(.{14})', line)\n",
    "#             print(ID)\n",
    "            URL = re.findall('(.+)&', line)\n",
    "#             print(URL)\n",
    "            toronto_URLs.append(URL[0])\n",
    "            toronto_IDs.append(ID[0])\n",
    "    f.close()\n",
    "    return (toronto_URLs, toronto_IDs)\n",
    "\n",
    "\n",
    "# Convert title to file system name\n",
    "def urlify_title(s):\n",
    "#     s = re.findall('(.+) - Data catalogue - Open Data', s)[0]\n",
    "    s = re.findall('(.+) - (.+) - (.+)$', s)[0][0]\n",
    "    s = re.sub(r'[^\\w\\s]', '', s) # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"\\s+\", '_', s) # Replace all runs of whitespace with a single dash\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get URLs to toronto.ca web pages, and toronto.ca IDs from the text file containing hyperlinks.\n",
    "(toronto_URLs, toronto_IDs) = get_toronto_urls('hyperlinks_toronto_chosen.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main loop for downloading data from toronto.ca\n",
    "\n",
    "for idx, toronto_URL in enumerate(toronto_URLs):\n",
    "    if idx < 5: continue\n",
    "    request = urllib.request.urlopen(toronto_URL)\n",
    "    html = request.read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    title = urlify_title(soup.title.string);\n",
    "    \n",
    "    print(\"\\n\",idx, \": Processing data source:\", title)\n",
    "    \n",
    "    # create metadata from original html\n",
    "    metadata = create_metadata(html)\n",
    "    \n",
    "    folder_path = os.path.join( my_base_path, title + '_' + toronto_IDs[idx] )\n",
    "    print(folder_path)\n",
    "    \n",
    "    # create folder to download files to\n",
    "    if not os.path.exists(folder_path):  os.makedirs(folder_path) \n",
    "\n",
    "    # download original html\n",
    "    orig_html_filename = toronto_IDs[idx] + '.html';\n",
    "    download_file(toronto_URLs[idx], folder_path, orig_html_filename)\n",
    "    \n",
    "    # download all data resources\n",
    "    clue_found = 0;\n",
    "    bad_count = 0;\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        href = str(link.get('href'))\n",
    "\n",
    "        if clue_found: # output all URLs and exit the loop at the next URL containing \"/wps/portal\"\n",
    "            if href.find('/wps/portal') >= 0:\n",
    "                if bad_count > 10: \n",
    "                    break\n",
    "                bad_count += 1\n",
    "            else:\n",
    "                if href.find('City',0,5) >= 0: # prepend 'http://www1.toronto.ca' to incomplete URLs starting with /City...\n",
    "                    href = 'http://www1.toronto.ca' + urllib.parse.quote(href)\n",
    "                print(\"  Downloading:\", href)\n",
    "                download_file(href, folder_path, rewrite = 0)\n",
    "                metadata['source_files'].append(href)\n",
    "        elif href.find('mailto:') >= 0: # when found \"mailto\" clue, take all URLs until the URL that starts with \"/wps/portal\"\n",
    "            clue_found = 1;\n",
    "    \n",
    "    # write metadata to disk\n",
    "    fp = open(os.path.join ( folder_path, 'metadata.json' ) , 'w');\n",
    "    \n",
    "    json.dump(metadata,fp)\n",
    "    fp.close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
